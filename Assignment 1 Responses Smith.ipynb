{"cells":[{"cell_type":"markdown","metadata":{"id":"nM60WzXg5823"},"source":["# Linear regression\n","\n","**Load data**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1706577489181,"user":{"displayName":"Yang Liu","userId":"12530007062946827169"},"user_tz":360},"id":"bYNKpo4556Em","outputId":"c15b3b6f-7167-4c40-d70e-e8d9dac6aa36"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pandas'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopt\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import scipy.optimize as opt\n","from matplotlib.colors import LogNorm\n","from mpl_toolkits.mplot3d import axes3d, Axes3D\n","import statsmodels.api as sm\n","\n","trees = sm.datasets.get_rdataset('trees').data\n","\n","df=trees\n","df =df.round(3)\n","df.head()\n","\n","y = df['Volume'].to_numpy()\n","m = y.size\n","X = df[['Girth', 'Height']].to_numpy()\n","# X.shape\n","\n","# Print out some data points\n","print('First 10 examples from the dataset: ')\n","for i in range(0, 10):\n","    print('x = {}, y = {}'.format(X[i], y[i]))\n","\n","trees.describe()"]},{"cell_type":"markdown","metadata":{"id":"q8rM5wyv6TtY"},"source":["**Complete the problems 1.1 - 1.4. Decompose your code into different sessions for clarity**"]},{"cell_type":"markdown","metadata":{},"source":["Problem 1.1: Visualizing the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 1.2: Define Loss Function and Perform Gradient\n","Descent "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 1.3: Exploring the Effect of Learning Rate and\n","feature normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 1.4: Comparing the GD Results with the ClosedForm Solution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"nfNUytsG6czi"},"source":["# Logistic regression\n","\n","**Load data**\n","\n","The data is stored in Google drive, this step mount the google drive and load the data through numpy.\n","\n","**Note, you need to change the path of the data file based on your case**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQQb9Pvt8l1H"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data = np.loadtxt('/content/drive/MyDrive/NUEN 689/Week 2/logisticRegression.txt', delimiter=',')\n","X = data[:, 0:2]\n","y = data[:, 2]"]},{"cell_type":"markdown","metadata":{"id":"fV2wj2MQ8006"},"source":["**Feature map function for Problem 2.5**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhtciTZH8kKJ"},"outputs":[],"source":["def map_feature(x1, x2):\n","    degree = 6\n","\n","    x1 = x1.reshape((x1.size, 1))\n","    x2 = x2.reshape((x2.size, 1))\n","    result = np.ones(x1[:, 0].shape)\n","\n","    for i in range(1, degree + 1):\n","        for j in range(0, i + 1):\n","            result = np.c_[result, (x1**(i-j)) * (x2**j)]\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"QOc5ifi781WF"},"source":["**Plot decision boundary**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fysMpk_W6f5S"},"outputs":[],"source":["def plot_decision_boundary(theta, X, y):\n","    plt.figure()\n","\n","    pos = np.where(y == 1)[0]\n","    neg = np.where(y == 0)[0]\n","\n","    plt.scatter(X[pos, 0], X[pos, 1], marker=\"+\", c='b')\n","    plt.scatter(X[neg, 0], X[neg, 1], marker=\"o\", c='y')\n","\n","    if X.shape[1] <= 3:\n","        # Only need two points to define a line, so choose two endpoints\n","        plot_x = np.array([np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2])\n","\n","        # Calculate the decision boundary line\n","        plot_y = (-1/theta[2]) * (theta[1]*plot_x + theta[0])\n","\n","        plt.plot(plot_x, plot_y)\n","\n","        plt.legend(['Decision Boundary', 'Admitted', 'Not admitted'], loc=1)\n","        plt.axis([30, 100, 30, 100])\n","    else:\n","        # Here is the grid range\n","        u = np.linspace(-1, 1.5, 50)\n","        v = np.linspace(-1, 1.5, 50)\n","\n","        z = np.zeros((u.size, v.size))\n","\n","        # Evaluate z = theta*x over the grid\n","        for i in range(0, u.size):\n","            for j in range(0, v.size):\n","                z[i, j] = np.dot(map_feature(u[i], v[j]), theta)\n","\n","        z = z.T\n","\n","        # Plot z = 0\n","        # Notice you need to specify the range [0, 0]\n","        cs = plt.contour(u, v, z, levels=[0], colors='r', label='Decision Boundary')\n","        plt.legend([cs.collections[0]], ['Decision Boundary'])"]},{"cell_type":"markdown","metadata":{"id":"w3Nx5EdJ-Sgi"},"source":["**Complete Problems 2.1-2.5, decompose your code into different sessions for clarity.**"]},{"cell_type":"markdown","metadata":{},"source":["Problem 2.1: Visualizing the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 2.2: Define Loss Function and Perform Gradient\n","Descent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 2.3: Comparing with the opt.fmin bfgs Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 2.4: Plotting the Decision Boundary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Problem 2.5: Feature Mapping and Regularization\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPwk4fVmFFmSjoze0egkA6X","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
